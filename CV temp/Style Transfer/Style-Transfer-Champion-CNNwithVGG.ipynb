{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b8ebf73-ca99-4001-8902-5c34acb0ac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Total loss 149.59\n",
      "Step 50, Total loss 11.42\n",
      "Step 100, Total loss 7.69\n",
      "Step 150, Total loss 6.17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------- Parameter Settings ---------\n",
    "content_img_path = \"E:/CV/content.jpg\"  # Replace with your own path\n",
    "style_img_path = \"E:/CV/style.jpg\"      # Replace with your own path\n",
    "output_img_path = \"E:/CV/output.jpg\"\n",
    "device = torch.device(\"cpu\")  # Use CPU only\n",
    "image_size = 512\n",
    "num_steps = 200  # Number of optimization steps\n",
    "tv_weight = 1e-6  # Total variation loss weight\n",
    "\n",
    "# --------- Image Preprocessing ---------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[:3, :, :]),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_image(path):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "def im_convert(tensor):\n",
    "    image = tensor.clone().detach()\n",
    "    image = image.squeeze(0)\n",
    "    image = image * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    image = image + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    image = image.clamp(0, 1)\n",
    "    return transforms.ToPILImage()(image)\n",
    "\n",
    "content = load_image(content_img_path)\n",
    "style = load_image(style_img_path)\n",
    "\n",
    "# --------- Feature Extraction Network ---------\n",
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatures, self).__init__()\n",
    "        self.vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "        self.layers = {'0': 'conv1_1', '5': 'conv2_1', \n",
    "               '10': 'conv3_1', '19': 'conv4_1', '28': 'conv5_1'}\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = {}\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.layers:\n",
    "                features[self.layers[name]] = x\n",
    "        return features\n",
    "\n",
    "# --------- Gram Matrix Computation ---------\n",
    "def gram_matrix(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    features = tensor.view(b * c, h * w)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(b * c * h * w)\n",
    "\n",
    "# --------- Initialization ---------\n",
    "model = VGGFeatures().to(device)\n",
    "target = torch.randn_like(content).to(device)\n",
    "target.requires_grad = True\n",
    "optimizer = optim.Adam([target], lr=0.05)\n",
    "style_features = model(style)\n",
    "content_features = model(content)\n",
    "\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "# Style and content weights\n",
    "style_weights = {\n",
    "    'conv1_1': 0.2,\n",
    "    'conv2_1': 0.2,\n",
    "    'conv3_1': 0.2,\n",
    "    'conv4_1': 0.2\n",
    "}\n",
    "content_weight = 1  # 降低内容权重\n",
    "style_weight = 1e5  # 提高风格权重\n",
    "# --------- Optimization Loop ---------\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    target_features = model(target)\n",
    "\n",
    "    # Content loss\n",
    "    content_loss = torch.mean((target_features['conv4_1'] - content_features['conv4_1'])**2)\n",
    "\n",
    "    # Style loss\n",
    "    style_loss = 0\n",
    "    for layer in style_weights:\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        style_gram = style_grams[layer]\n",
    "        style_loss += style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "\n",
    "    # Total variation loss for smoothness\n",
    "    tv_loss = torch.sum(torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])) + \\\n",
    "              torch.sum(torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :]))\n",
    "\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss + tv_weight * tv_loss\n",
    "    total_loss.backward(retain_graph=True) \n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}, Total loss {total_loss.item():.2f}\")\n",
    "\n",
    "# --------- Save and Display Output ---------\n",
    "output_img = im_convert(target)\n",
    "output_img.save(output_img_path)\n",
    "output_img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b8c05-9b9c-442e-afdb-1711f36342ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Step 20, Total loss: 16.94\n",
      "Step 40, Total loss: 13.13\n",
      "Step 60, Total loss: 12.02\n",
      "Step 80, Total loss: 11.50\n",
      "Step 100, Total loss: 11.18\n",
      "Step 120, Total loss: 10.97\n",
      "Step 140, Total loss: 10.82\n",
      "Step 160, Total loss: 10.71\n",
      "Step 180, Total loss: 10.63\n",
      "Step 200, Total loss: 10.56\n",
      "Step 220, Total loss: 10.50\n",
      "Step 240, Total loss: 10.45\n",
      "Step 260, Total loss: 10.41\n",
      "Step 280, Total loss: 10.38\n",
      "Step 300, Total loss: 10.35\n",
      "Step 320, Total loss: 10.32\n",
      "Step 340, Total loss: 10.30\n",
      "Step 360, Total loss: 10.28\n",
      "Step 380, Total loss: 10.26\n",
      "Step 400, Total loss: 10.24\n",
      "Step 420, Total loss: 10.22\n",
      "Step 440, Total loss: 10.21\n",
      "Step 460, Total loss: 10.20\n",
      "Step 480, Total loss: 10.18\n",
      "Step 500, Total loss: 10.17\n",
      "Step 520, Total loss: 10.16\n",
      "Step 540, Total loss: 10.15\n",
      "Step 560, Total loss: 10.14\n",
      "Step 580, Total loss: 10.14\n",
      "Step 600, Total loss: 10.13\n",
      "Created upscaled version at E:/CV/output_upscaled.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------- Parameter Settings ---------\n",
    "content_img_path = \"E:/CV/content.jpg\"  # Replace with your own path\n",
    "style_img_path = \"E:/CV/style.jpg\"      # Replace with your own path\n",
    "output_img_path = \"E:/CV/output.jpg\"\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Improved parameters for better quality\n",
    "image_size = 512  # You can increase this for higher resolution (e.g., 1024)\n",
    "num_steps = 300   # More steps for better convergence\n",
    "tv_weight = 1e-6  # Total variation loss weight\n",
    "\n",
    "# --------- Image Preprocessing ---------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[:3, :, :]),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_image(path):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "def im_convert(tensor):\n",
    "    image = tensor.clone().detach().cpu()  # Move to CPU for PIL conversion\n",
    "    image = image.squeeze(0)\n",
    "    image = image * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    image = image + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    image = image.clamp(0, 1)\n",
    "    return transforms.ToPILImage()(image)\n",
    "\n",
    "# Load images\n",
    "content = load_image(content_img_path)\n",
    "style = load_image(style_img_path)\n",
    "\n",
    "# --------- Feature Extraction Network ---------\n",
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatures, self).__init__()\n",
    "        self.vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "        # Freeze VGG parameters\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        self.layers = {'0': 'conv1_1', '5': 'conv2_1', \n",
    "               '10': 'conv3_1', '19': 'conv4_1', '28': 'conv5_1'}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = {}\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.layers:\n",
    "                features[self.layers[name]] = x\n",
    "        return features\n",
    "\n",
    "# --------- Gram Matrix Computation ---------\n",
    "def gram_matrix(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    features = tensor.view(b * c, h * w)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(b * c * h * w)\n",
    "\n",
    "# --------- Initialization ---------\n",
    "model = VGGFeatures().to(device)\n",
    "\n",
    "# Initialize with content image for faster convergence\n",
    "target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "# Use LBFGS optimizer for better results\n",
    "optimizer = optim.LBFGS([target])\n",
    "\n",
    "style_features = model(style)\n",
    "content_features = model(content)\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "# Style and content weights\n",
    "style_weights = {\n",
    "    'conv1_1': 0.75,  # More weight on lower layers for fine details\n",
    "    'conv2_1': 0.5,\n",
    "    'conv3_1': 0.2,\n",
    "    'conv4_1': 0.2,\n",
    "    'conv5_1': 0.2,   # Added deeper layer\n",
    "}\n",
    "\n",
    "content_weight = 1      \n",
    "style_weight = 1e6      # Increased style weight for better stylization\n",
    "\n",
    "# --------- Optimization Loop ---------\n",
    "step = [0]  # Use a list to track step in closure\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    target_features = model(target)\n",
    "    \n",
    "    # Content loss - only use one deeper layer\n",
    "    content_loss = torch.mean((target_features['conv4_1'] - content_features['conv4_1'])**2)\n",
    "    \n",
    "    # Style loss\n",
    "    style_loss = 0\n",
    "    for layer in style_weights:\n",
    "        if layer in target_features:  # Make sure layer exists\n",
    "            target_feature = target_features[layer]\n",
    "            target_gram = gram_matrix(target_feature)\n",
    "            style_gram = style_grams[layer]\n",
    "            layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            style_loss += layer_style_loss\n",
    "    \n",
    "    # Total variation loss for smoothness\n",
    "    tv_loss = torch.sum(torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])) + \\\n",
    "              torch.sum(torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :]))\n",
    "    \n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss + tv_weight * tv_loss\n",
    "    total_loss.backward()\n",
    "    \n",
    "    step[0] += 1\n",
    "    if step[0] % 20 == 0:\n",
    "        print(f\"Step {step[0]}, Total loss: {total_loss.item():.2f}\")\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# Run optimization with LBFGS\n",
    "for i in range(num_steps // 10):  # LBFGS takes bigger steps\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    # Save intermediate results\n",
    "    if (i+1) % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            output_img = im_convert(target)\n",
    "            output_img.save(f\"E:/CV/output_step_{(i+1)*10}.jpg\")\n",
    "\n",
    "# --------- Save and Display Output ---------\n",
    "output_img = im_convert(target)\n",
    "output_img.save(output_img_path)\n",
    "\n",
    "# Option to upscale the final image for better clarity\n",
    "try:\n",
    "    from PIL import Image\n",
    "    final_img = Image.open(output_img_path)\n",
    "    upscaled_img = final_img.resize((final_img.width*2, final_img.height*2), Image.LANCZOS)\n",
    "    upscaled_img.save(\"E:/CV/output_upscaled.jpg\")\n",
    "    print(\"Created upscaled version at E:/CV/output_upscaled.jpg\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create upscaled version: {e}\")\n",
    "\n",
    "# Display the final image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(output_img)\n",
    "plt.title(\"Final Style Transfer Result\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a4f78-c86c-46a9-b3b5-2712225bef48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
