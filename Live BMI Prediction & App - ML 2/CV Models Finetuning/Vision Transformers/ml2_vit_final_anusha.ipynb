{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc66b5f",
   "metadata": {
    "id": "cbc66b5f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torchvision.models import vit_b_16\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import LinearSVR\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CGeuCAVy_sQb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22466,
     "status": "ok",
     "timestamp": 1748041241345,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "CGeuCAVy_sQb",
    "outputId": "868cd998-53e2-47d5-bc9f-5962460f3b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aiumQIGyh61z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1748043919335,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "aiumQIGyh61z",
    "outputId": "2aaeb48a-f535-4cdc-e39e-a502b4dc536c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid samples after filtering missing images: 3962\n"
     ]
    }
   ],
   "source": [
    "# === Paths ===\n",
    "drive_root = \"/content/drive/MyDrive/BMI/Data\"\n",
    "csv_path = os.path.join(drive_root, \"data.csv\")\n",
    "image_folder = os.path.join(drive_root, \"Images\")\n",
    "\n",
    "# === Load CSV metadata ===\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# === Filter out missing images upfront ===\n",
    "df = df[df['name'].apply(lambda x: os.path.isfile(os.path.join(image_folder, x)))].reset_index(drop=True)\n",
    "print(f\"Valid samples after filtering missing images: {len(df)}\")\n",
    "\n",
    "# === Prepare file paths and labels ===\n",
    "image_paths = [os.path.join(image_folder, fname) for fname in df['name']]\n",
    "bmi_labels = df['bmi'].values.astype(np.float32)\n",
    "gender_map = {'Male': 0, 'Female': 1}\n",
    "gender_labels = df['gender'].map(gender_map).values.astype(np.int64)\n",
    "\n",
    "# === Define transform (ViT style) ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# === Split indices for train/test (80/20) ===\n",
    "num_samples = len(image_paths)\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.8 * num_samples)\n",
    "train_indices, test_indices = indices[:split], indices[split:]\n",
    "\n",
    "# === Create DataLoader that yields indices ===\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_indices, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_indices, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RQqenTvIihZ_",
   "metadata": {
    "id": "RQqenTvIihZ_"
   },
   "outputs": [],
   "source": [
    "# === Function to load a batch of images and labels given indices ===\n",
    "def load_batch(batch_indices):\n",
    "    batch_images = []\n",
    "    batch_bmis = []\n",
    "    batch_genders = []\n",
    "    for idx in batch_indices:\n",
    "        img = Image.open(image_paths[idx]).convert('RGB')\n",
    "        img = transform(img)\n",
    "        batch_images.append(img)\n",
    "        batch_bmis.append(bmi_labels[idx])\n",
    "        batch_genders.append(gender_labels[idx])\n",
    "    images_tensor = torch.stack(batch_images)\n",
    "    bmi_tensor = torch.tensor(batch_bmis).unsqueeze(1)  # shape [batch, 1]\n",
    "    gender_tensor = torch.tensor(batch_genders)\n",
    "    return images_tensor, bmi_tensor, gender_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24tyL23OjR7T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5307,
     "status": "ok",
     "timestamp": 1748044174877,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "24tyL23OjR7T",
    "outputId": "e8527023-664a-4a6c-d286-cac01f5a02fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
      "100%|██████████| 330M/330M [00:01<00:00, 189MB/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load pretrained ViT\n",
    "model = vit_b_16(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Freeze all layers first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze final transformer block and head\n",
    "for name, param in model.encoder.layers[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.heads.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Replace classification head with regression head (output 1 BMI value)\n",
    "model.heads = nn.Sequential(\n",
    "    nn.Linear(model.heads.head.in_features, 1)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E43ObLt2h9e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1371159,
     "status": "ok",
     "timestamp": 1748045567332,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "E43ObLt2h9e0",
    "outputId": "77441fa9-b6c1-4237-902b-d1b39ecefe9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 521.5161\n",
      "Epoch [2/10], Loss: 303.2960\n",
      "Epoch [3/10], Loss: 193.5199\n",
      "Epoch [4/10], Loss: 126.9845\n",
      "Epoch [5/10], Loss: 93.7366\n",
      "Epoch [6/10], Loss: 77.9454\n",
      "Epoch [7/10], Loss: 71.7088\n",
      "Epoch [8/10], Loss: 70.1345\n",
      "Epoch [9/10], Loss: 67.3945\n",
      "Epoch [10/10], Loss: 61.7492\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_indices in train_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "        images = images.to(device)\n",
    "        bmis = bmis.to(device)\n",
    "        # genders can be used later if you want, but not needed here\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, bmis)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-mWKSdtuo3r7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196417,
     "status": "ok",
     "timestamp": 1748046141055,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "-mWKSdtuo3r7",
    "outputId": "5d848047-b408-40c3-cfb9-5126c5d4f098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Pearson r: 0.5151\n",
      "Male Pearson r: 0.5363\n",
      "Female Pearson r: 0.4956\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_genders = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_indices in test_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "\n",
    "        images = images.to(device)\n",
    "        outputs = model(images).cpu().squeeze().numpy()\n",
    "\n",
    "        all_preds.extend(outputs)\n",
    "        all_labels.extend(bmis.squeeze().numpy())\n",
    "        all_genders.extend(genders.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_genders = np.array(all_genders)\n",
    "\n",
    "# Overall Pearson correlation\n",
    "overall_r, _ = pearsonr(all_preds, all_labels)\n",
    "print(f\"Overall Pearson r: {overall_r:.4f}\")\n",
    "\n",
    "# Pearson r for males (gender 0)\n",
    "male_mask = (all_genders == 0)\n",
    "male_r, _ = pearsonr(all_preds[male_mask], all_labels[male_mask])\n",
    "print(f\"Male Pearson r: {male_r:.4f}\")\n",
    "\n",
    "# Pearson r for females (gender 1)\n",
    "female_mask = (all_genders == 1)\n",
    "female_r, _ = pearsonr(all_preds[female_mask], all_labels[female_mask])\n",
    "print(f\"Female Pearson r: {female_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-QS15WlXrQu-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307015,
     "status": "ok",
     "timestamp": 1748046580869,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "-QS15WlXrQu-",
    "outputId": "bb0b5104-135a-41ec-a34a-9040792e4382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 54.7205\n",
      "Epoch [2/5], Loss: 49.3816\n",
      "Epoch [3/5], Loss: 43.8568\n",
      "Epoch [4/5], Loss: 39.5742\n",
      "Epoch [5/5], Loss: 35.7136\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_indices in train_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "        images = images.to(device)\n",
    "        bmis = bmis.to(device)\n",
    "        # genders can be used later if you want, but not needed here\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, bmis)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fQrFbDUGshbk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12631,
     "status": "ok",
     "timestamp": 1748046617745,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "fQrFbDUGshbk",
    "outputId": "684d1e4d-9fc9-4d1c-d379-8bb9e1cfcbda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Pearson r: 0.6052\n",
      "Male Pearson r: 0.6535\n",
      "Female Pearson r: 0.5523\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_genders = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_indices in test_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "\n",
    "        images = images.to(device)\n",
    "        outputs = model(images).cpu().squeeze().numpy()\n",
    "\n",
    "        all_preds.extend(outputs)\n",
    "        all_labels.extend(bmis.squeeze().numpy())\n",
    "        all_genders.extend(genders.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_genders = np.array(all_genders)\n",
    "\n",
    "# Overall Pearson correlation\n",
    "overall_r, _ = pearsonr(all_preds, all_labels)\n",
    "print(f\"Overall Pearson r: {overall_r:.4f}\")\n",
    "\n",
    "# Pearson r for males (gender 0)\n",
    "male_mask = (all_genders == 0)\n",
    "male_r, _ = pearsonr(all_preds[male_mask], all_labels[male_mask])\n",
    "print(f\"Male Pearson r: {male_r:.4f}\")\n",
    "\n",
    "# Pearson r for females (gender 1)\n",
    "female_mask = (all_genders == 1)\n",
    "female_r, _ = pearsonr(all_preds[female_mask], all_labels[female_mask])\n",
    "print(f\"Female Pearson r: {female_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lm1v79kKszl3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 612363,
     "status": "ok",
     "timestamp": 1748047279623,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "Lm1v79kKszl3",
    "outputId": "2b6861cd-f01d-4564-8ab4-4d64627f10e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 32.7932\n",
      "Epoch [2/10], Loss: 31.5823\n",
      "Epoch [3/10], Loss: 31.3000\n",
      "Epoch [4/10], Loss: 25.6771\n",
      "Epoch [5/10], Loss: 23.2953\n",
      "Epoch [6/10], Loss: 22.5334\n",
      "Epoch [7/10], Loss: 19.2481\n",
      "Epoch [8/10], Loss: 17.9577\n",
      "Epoch [9/10], Loss: 16.1079\n",
      "Epoch [10/10], Loss: 16.0394\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_indices in train_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "        images = images.to(device)\n",
    "        bmis = bmis.to(device)\n",
    "        # genders can be used later if you want, but not needed here\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, bmis)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yyFhdHhJvOa3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12775,
     "status": "ok",
     "timestamp": 1748047307493,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "yyFhdHhJvOa3",
    "outputId": "9d4faa64-f0bd-47b6-d16c-9a1d4862c6d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Pearson r: 0.6076\n",
      "Male Pearson r: 0.6687\n",
      "Female Pearson r: 0.5374\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_genders = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_indices in test_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "\n",
    "        images = images.to(device)\n",
    "        outputs = model(images).cpu().squeeze().numpy()\n",
    "\n",
    "        all_preds.extend(outputs)\n",
    "        all_labels.extend(bmis.squeeze().numpy())\n",
    "        all_genders.extend(genders.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_genders = np.array(all_genders)\n",
    "\n",
    "# Overall Pearson correlation\n",
    "overall_r, _ = pearsonr(all_preds, all_labels)\n",
    "print(f\"Overall Pearson r: {overall_r:.4f}\")\n",
    "\n",
    "# Pearson r for males (gender 0)\n",
    "male_mask = (all_genders == 0)\n",
    "male_r, _ = pearsonr(all_preds[male_mask], all_labels[male_mask])\n",
    "print(f\"Male Pearson r: {male_r:.4f}\")s\n",
    "\n",
    "# Pearson r for females (gender 1)\n",
    "female_mask = (all_genders == 1)\n",
    "female_r, _ = pearsonr(all_preds[female_mask], all_labels[female_mask])\n",
    "print(f\"Female Pearson r: {female_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-hEky62gvt35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306462,
     "status": "ok",
     "timestamp": 1748047735724,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "-hEky62gvt35",
    "outputId": "2680794d-177a-4a19-b7fb-b5cf38db725c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 13.7384\n",
      "Epoch [2/5], Loss: 12.2806\n",
      "Epoch [3/5], Loss: 10.6522\n",
      "Epoch [4/5], Loss: 9.7092\n",
      "Epoch [5/5], Loss: 8.6710\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_indices in train_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "        images = images.to(device)\n",
    "        bmis = bmis.to(device)\n",
    "        # genders can be used later if you want, but not needed here\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, bmis)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qYkoD2oFv0PK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12456,
     "status": "ok",
     "timestamp": 1748047756458,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "qYkoD2oFv0PK",
    "outputId": "fd9d6c62-b40c-4d86-a5c7-4ee4a0a5674c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Pearson r: 0.5907\n",
      "Male Pearson r: 0.6508\n",
      "Female Pearson r: 0.5164\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_genders = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_indices in test_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "\n",
    "        images = images.to(device)\n",
    "        outputs = model(images).cpu().squeeze().numpy()\n",
    "\n",
    "        all_preds.extend(outputs)\n",
    "        all_labels.extend(bmis.squeeze().numpy())\n",
    "        all_genders.extend(genders.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_genders = np.array(all_genders)\n",
    "\n",
    "# Overall Pearson correlation\n",
    "overall_r, _ = pearsonr(all_preds, all_labels)\n",
    "print(f\"Overall Pearson r: {overall_r:.4f}\")\n",
    "\n",
    "# Pearson r for males (gender 0)\n",
    "male_mask = (all_genders == 0)\n",
    "male_r, _ = pearsonr(all_preds[male_mask], all_labels[male_mask])\n",
    "print(f\"Male Pearson r: {male_r:.4f}\")\n",
    "\n",
    "# Pearson r for females (gender 1)\n",
    "female_mask = (all_genders == 1)\n",
    "female_r, _ = pearsonr(all_preds[female_mask], all_labels[female_mask])\n",
    "print(f\"Female Pearson r: {female_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qP1mv-wBxR9v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1748048027423,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "qP1mv-wBxR9v",
    "outputId": "0a19b35d-6913-48f2-e6d1-d3281ecdefd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained ViT\n",
    "model2 = vit_b_16(weights='IMAGENET1K_V1')\n",
    "\n",
    "# === Step 1: Freeze all layers\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# === Step 2: Unfreeze last two transformer blocks\n",
    "for name, param in model2.encoder.layers[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "for name, param in model2.encoder.layers[-2].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# === Step 3: Replace and unfreeze the classification head for regression\n",
    "model2.heads = nn.Sequential(\n",
    "    nn.Linear(model2.heads.head.in_features, 1)\n",
    ")\n",
    "for param in model2.heads.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# === Step 4: Set up training components\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model2.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PaSd8HaYyC7v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 996144,
     "status": "ok",
     "timestamp": 1748049057937,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "PaSd8HaYyC7v",
    "outputId": "55f42a70-29be-4ead-ca67-ae8381bf7a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 501.1352\n",
      "Epoch [2/15], Loss: 301.6849\n",
      "Epoch [3/15], Loss: 191.9251\n",
      "Epoch [4/15], Loss: 126.9388\n",
      "Epoch [5/15], Loss: 93.7092\n",
      "Epoch [6/15], Loss: 78.0076\n",
      "Epoch [7/15], Loss: 71.6919\n",
      "Epoch [8/15], Loss: 70.0500\n",
      "Epoch [9/15], Loss: 66.7818\n",
      "Epoch [10/15], Loss: 61.1880\n",
      "Epoch [11/15], Loss: 53.5123\n",
      "Epoch [12/15], Loss: 48.8926\n",
      "Epoch [13/15], Loss: 43.6410\n",
      "Epoch [14/15], Loss: 40.3833\n",
      "Epoch [15/15], Loss: 36.0550\n"
     ]
    }
   ],
   "source": [
    "# === Training loop for model2 ===\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_indices in train_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "        images, bmis = images.to(device), bmis.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(images)\n",
    "        loss = criterion(outputs, bmis)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3YCoR2uE2B3O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12634,
     "status": "ok",
     "timestamp": 1748049127660,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "3YCoR2uE2B3O",
    "outputId": "36f2238a-a93b-40c3-d9fc-d289ec89c43e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Pearson r: 0.5679\n",
      "Male Pearson r: 0.6091\n",
      "Female Pearson r: 0.5177\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_genders = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_indices in test_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "\n",
    "        images = images.to(device)\n",
    "        outputs = model2(images).cpu().squeeze().numpy()\n",
    "\n",
    "        all_preds.extend(outputs)\n",
    "        all_labels.extend(bmis.squeeze().numpy())\n",
    "        all_genders.extend(genders.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_genders = np.array(all_genders)\n",
    "\n",
    "# Overall Pearson correlation\n",
    "overall_r, _ = pearsonr(all_preds, all_labels)\n",
    "print(f\"Overall Pearson r: {overall_r:.4f}\")\n",
    "\n",
    "# Pearson r for males (gender 0)\n",
    "male_mask = (all_genders == 0)\n",
    "male_r, _ = pearsonr(all_preds[male_mask], all_labels[male_mask])\n",
    "print(f\"Male Pearson r: {male_r:.4f}\")\n",
    "\n",
    "# Pearson r for females (gender 1)\n",
    "female_mask = (all_genders == 1)\n",
    "female_r, _ = pearsonr(all_preds[female_mask], all_labels[female_mask])\n",
    "print(f\"Female Pearson r: {female_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amBAzNCc2b_P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1062,
     "status": "ok",
     "timestamp": 1748049729054,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "amBAzNCc2b_P",
    "outputId": "9da29f87-214e-4fea-de29-246481b6da64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained ViT\n",
    "model3 = vit_b_16(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Unfreeze last 3 transformer blocks + heads\n",
    "for param in model3.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for i in range(-3, 0):\n",
    "    for param in model3.encoder.layers[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in model3.heads.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# === Step 3: Replace and unfreeze the classification head for regression\n",
    "model3.heads = nn.Sequential(\n",
    "    nn.Linear(model3.heads.head.in_features, 1)\n",
    ")\n",
    "for param in model3.heads.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# === Step 4: Set up training components\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model3.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=15)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oBoWMmRe3Is-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1077140,
     "status": "ok",
     "timestamp": 1748050815736,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "oBoWMmRe3Is-",
    "outputId": "967c6d0f-35bd-4d4d-ccb6-6ade767fbbb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 487.4022\n",
      "Epoch [2/15], Loss: 299.9130\n",
      "Epoch [3/15], Loss: 193.6574\n",
      "Epoch [4/15], Loss: 133.3260\n",
      "Epoch [5/15], Loss: 96.9115\n",
      "Epoch [6/15], Loss: 81.5277\n",
      "Epoch [7/15], Loss: 74.6889\n",
      "Epoch [8/15], Loss: 71.9249\n",
      "Epoch [9/15], Loss: 71.6284\n",
      "Epoch [10/15], Loss: 77.1209\n",
      "Epoch [11/15], Loss: 69.3188\n",
      "Epoch [12/15], Loss: 68.8281\n",
      "Epoch [13/15], Loss: 67.4523\n",
      "Epoch [14/15], Loss: 63.4685\n",
      "Epoch [15/15], Loss: 62.0729\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "# === Training loop for model3 ===\n",
    "for epoch in range(num_epochs):\n",
    "    model3.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_indices in train_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "        images, bmis = images.to(device), bmis.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model3(images)\n",
    "        loss = criterion(outputs, bmis)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()  # Step the learning rate scheduler after each epoch\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KiByAu8i8a84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12798,
     "status": "ok",
     "timestamp": 1748050834309,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "KiByAu8i8a84",
    "outputId": "4f2f0c78-e082-473b-8162-9c7a94151c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Pearson r: 0.4427\n",
      "Male Pearson r: 0.4594\n",
      "Female Pearson r: 0.4301\n"
     ]
    }
   ],
   "source": [
    "model3.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_genders = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_indices in test_loader:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "\n",
    "        images = images.to(device)\n",
    "        outputs = model3(images).cpu().squeeze().numpy()\n",
    "\n",
    "        all_preds.extend(outputs)\n",
    "        all_labels.extend(bmis.squeeze().numpy())\n",
    "        all_genders.extend(genders.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_genders = np.array(all_genders)\n",
    "\n",
    "# Overall Pearson correlation\n",
    "overall_r, _ = pearsonr(all_preds, all_labels)\n",
    "print(f\"Overall Pearson r: {overall_r:.4f}\")\n",
    "\n",
    "# Pearson r for males (gender 0)\n",
    "male_mask = (all_genders == 0)\n",
    "male_r, _ = pearsonr(all_preds[male_mask], all_labels[male_mask])\n",
    "print(f\"Male Pearson r: {male_r:.4f}\")\n",
    "\n",
    "# Pearson r for females (gender 1)\n",
    "female_mask = (all_genders == 1)\n",
    "female_r, _ = pearsonr(all_preds[female_mask], all_labels[female_mask])\n",
    "print(f\"Female Pearson r: {female_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9jwypHX-Kv6",
   "metadata": {
    "id": "s9jwypHX-Kv6"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Split 20% of train_indices into validation\n",
    "val_split = int(0.8 * len(train_indices))\n",
    "final_train_indices = train_indices[:val_split]\n",
    "val_indices = train_indices[val_split:]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader2 = DataLoader(final_train_indices, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader2 = DataLoader(val_indices, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader2 = DataLoader(test_indices, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inKmGsMe-uby",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1748051576158,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "inKmGsMe-uby",
    "outputId": "0f4dac53-0b3d-4a44-883f-27912fb15331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2535\n",
      "Val samples: 634\n",
      "Test samples: 793\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(final_train_indices)}\")\n",
    "print(f\"Val samples: {len(val_indices)}\")\n",
    "print(f\"Test samples: {len(test_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rV5nNVeG-v6a",
   "metadata": {
    "id": "rV5nNVeG-v6a"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model (using your vit_b_16 or timm vit_base_patch16_224)\n",
    "model4 = vit_b_16(weights='IMAGENET1K_V1')\n",
    "for param in model4.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last transformer block and head\n",
    "for param in model4.encoder.layers[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model4.heads.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model4.heads = nn.Linear(model4.heads.head.in_features, 1)\n",
    "model4.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model4.parameters()),\n",
    "    lr=1e-5,\n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CqZ1fpEH_Ulq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3033661,
     "status": "ok",
     "timestamp": 1748054677065,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "CqZ1fpEH_Ulq",
    "outputId": "eaafca5e-7aeb-4054-a7a8-e3f8d99379b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 963.6217, Val Loss: 781.5673\n",
      "Epoch 2, Train Loss: 664.4282, Val Loss: 587.8401\n",
      "Epoch 3, Train Loss: 554.8544, Val Loss: 528.7000\n",
      "Epoch 4, Train Loss: 512.1087, Val Loss: 496.9226\n",
      "Epoch 5, Train Loss: 485.4034, Val Loss: 473.8698\n",
      "Epoch 6, Train Loss: 464.4980, Val Loss: 454.7173\n",
      "Epoch 7, Train Loss: 446.3294, Val Loss: 437.4102\n",
      "Epoch 8, Train Loss: 429.7449, Val Loss: 421.3925\n",
      "Epoch 9, Train Loss: 414.1753, Val Loss: 406.1804\n",
      "Epoch 10, Train Loss: 399.3932, Val Loss: 391.7116\n",
      "Epoch 11, Train Loss: 385.2322, Val Loss: 377.8428\n",
      "Epoch 12, Train Loss: 371.6400, Val Loss: 364.3631\n",
      "Epoch 13, Train Loss: 358.4650, Val Loss: 351.4430\n",
      "Epoch 14, Train Loss: 345.7294, Val Loss: 338.8569\n",
      "Epoch 15, Train Loss: 333.4128, Val Loss: 326.7060\n",
      "Epoch 16, Train Loss: 321.5056, Val Loss: 315.0058\n",
      "Epoch 17, Train Loss: 309.9238, Val Loss: 303.5757\n",
      "Epoch 18, Train Loss: 298.6616, Val Loss: 292.4423\n",
      "Epoch 19, Train Loss: 287.8139, Val Loss: 281.7512\n",
      "Epoch 20, Train Loss: 277.2817, Val Loss: 271.4030\n",
      "Epoch 21, Train Loss: 267.0894, Val Loss: 261.3702\n",
      "Epoch 22, Train Loss: 257.2265, Val Loss: 251.6612\n",
      "Epoch 23, Train Loss: 247.6651, Val Loss: 242.2049\n",
      "Epoch 24, Train Loss: 238.4527, Val Loss: 233.1663\n",
      "Epoch 25, Train Loss: 229.5561, Val Loss: 224.3906\n",
      "Epoch 26, Train Loss: 220.9856, Val Loss: 215.9950\n",
      "Epoch 27, Train Loss: 212.7094, Val Loss: 207.8544\n",
      "Epoch 28, Train Loss: 204.7650, Val Loss: 200.0106\n",
      "Epoch 29, Train Loss: 197.0989, Val Loss: 192.4743\n",
      "Epoch 30, Train Loss: 189.7307, Val Loss: 185.2348\n",
      "Epoch 31, Train Loss: 182.6449, Val Loss: 178.3069\n",
      "Epoch 32, Train Loss: 175.8266, Val Loss: 171.6093\n",
      "Epoch 33, Train Loss: 169.2810, Val Loss: 165.1756\n",
      "Epoch 34, Train Loss: 163.0406, Val Loss: 159.0660\n",
      "Epoch 35, Train Loss: 157.0862, Val Loss: 153.2678\n",
      "Epoch 36, Train Loss: 151.4099, Val Loss: 147.6817\n",
      "Epoch 37, Train Loss: 145.9823, Val Loss: 142.3517\n",
      "Epoch 38, Train Loss: 140.7918, Val Loss: 137.3208\n",
      "Epoch 39, Train Loss: 135.8763, Val Loss: 132.4839\n",
      "Epoch 40, Train Loss: 131.1684, Val Loss: 127.8699\n",
      "Epoch 41, Train Loss: 126.6918, Val Loss: 123.5270\n",
      "Epoch 42, Train Loss: 122.4608, Val Loss: 119.3955\n",
      "Epoch 43, Train Loss: 118.4468, Val Loss: 115.4716\n",
      "Epoch 44, Train Loss: 114.6203, Val Loss: 111.7604\n",
      "Epoch 45, Train Loss: 111.0182, Val Loss: 108.2363\n",
      "Epoch 46, Train Loss: 107.6311, Val Loss: 104.9485\n",
      "Epoch 47, Train Loss: 104.4692, Val Loss: 101.8714\n",
      "Epoch 48, Train Loss: 101.5061, Val Loss: 98.9876\n",
      "Epoch 49, Train Loss: 98.7145, Val Loss: 96.3146\n",
      "Epoch 50, Train Loss: 96.1103, Val Loss: 93.7387\n"
     ]
    }
   ],
   "source": [
    "# Training loop (with validation)\n",
    "num_epochs = 50  # allow early stop\n",
    "best_val_loss = float('inf')\n",
    "patience = 7\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model4.train()\n",
    "    train_loss = 0\n",
    "    for batch_indices in train_loader2:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, _ = load_batch(batch_indices)\n",
    "        images, bmis = images.to(device), bmis.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model4(images)\n",
    "        loss = criterion(outputs, bmis)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_loader2.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model4.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_indices in val_loader2:\n",
    "            batch_indices = batch_indices.numpy()\n",
    "            images, bmis, _ = load_batch(batch_indices)\n",
    "            images, bmis = images.to(device), bmis.to(device)\n",
    "            outputs = model4(images)\n",
    "            loss = criterion(outputs, bmis)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    val_loss /= len(val_loader2.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model4.state_dict(), 'best_model.pth')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m_OL_o8lLpPV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1641,
     "status": "ok",
     "timestamp": 1748054872517,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "m_OL_o8lLpPV",
    "outputId": "2398eb7c-94bd-4602-b92b-3622769b037f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 119MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Load a pre-trained ResNet50 model\n",
    "model5 = models.resnet50(weights = ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Replace the final fully connected layer for regression\n",
    "model5.fc = nn.Sequential(\n",
    "    nn.Linear(model5.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, 1)  # Single output: predicted BMI\n",
    ")\n",
    "\n",
    "# Fine-tune entire network or freeze early layers\n",
    "for param in model5.parameters():\n",
    "    param.requires_grad = True  # Or freeze first few layers if you want\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.SmoothL1Loss()  # Huber loss\n",
    "optimizer = torch.optim.Adam(model5.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jfYJYmH_MRXL",
   "metadata": {
    "id": "jfYJYmH_MRXL"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model5 = model5.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GWX3dY0jMYTC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458816,
     "status": "ok",
     "timestamp": 1748055822112,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "GWX3dY0jMYTC",
    "outputId": "7486dcad-a135-47a8-e68e-ef732e1d589b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 80/80 [00:39<00:00,  2.00it/s]\n",
      "Epoch 1 - Validation: 100%|██████████| 20/20 [00:06<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.9452, Val Loss: 4.2420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|██████████| 80/80 [00:40<00:00,  1.99it/s]\n",
      "Epoch 2 - Validation: 100%|██████████| 20/20 [00:05<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 2.8156, Val Loss: 4.1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|██████████| 80/80 [00:40<00:00,  2.00it/s]\n",
      "Epoch 3 - Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 2.6259, Val Loss: 4.1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|██████████| 80/80 [00:39<00:00,  2.02it/s]\n",
      "Epoch 4 - Validation: 100%|██████████| 20/20 [00:05<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 2.4986, Val Loss: 4.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training: 100%|██████████| 80/80 [00:39<00:00,  2.00it/s]\n",
      "Epoch 5 - Validation: 100%|██████████| 20/20 [00:06<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 2.3226, Val Loss: 4.1420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training: 100%|██████████| 80/80 [00:39<00:00,  2.00it/s]\n",
      "Epoch 6 - Validation: 100%|██████████| 20/20 [00:05<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 2.4176, Val Loss: 4.1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training: 100%|██████████| 80/80 [00:39<00:00,  2.01it/s]\n",
      "Epoch 7 - Validation: 100%|██████████| 20/20 [00:06<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 2.2354, Val Loss: 4.1224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training: 100%|██████████| 80/80 [00:39<00:00,  2.00it/s]\n",
      "Epoch 8 - Validation: 100%|██████████| 20/20 [00:05<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 2.1203, Val Loss: 4.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training: 100%|██████████| 80/80 [00:39<00:00,  2.01it/s]\n",
      "Epoch 9 - Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 2.0969, Val Loss: 4.0247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training: 100%|██████████| 80/80 [00:39<00:00,  2.01it/s]\n",
      "Epoch 10 - Validation: 100%|██████████| 20/20 [00:05<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.1079, Val Loss: 3.9137\n"
     ]
    }
   ],
   "source": [
    "# === Training configuration ===\n",
    "num_epochs = 10\n",
    "patience = 7\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # === Training ===\n",
    "    model5.train()\n",
    "    train_loss = 0\n",
    "    for batch_indices in tqdm(train_loader2, desc=f\"Epoch {epoch+1} - Training\"):\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, _ = load_batch(batch_indices)\n",
    "        images, bmis = images.to(device), bmis.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model5(images)\n",
    "        loss = criterion(outputs, bmis)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_loader2.dataset)\n",
    "\n",
    "    # === Validation ===\n",
    "    model5.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_indices in tqdm(val_loader2, desc=f\"Epoch {epoch+1} - Validation\"):\n",
    "            batch_indices = batch_indices.numpy()\n",
    "            images, bmis, _ = load_batch(batch_indices)\n",
    "            images, bmis = images.to(device), bmis.to(device)\n",
    "\n",
    "            outputs = model5(images)\n",
    "            loss = criterion(outputs, bmis)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    val_loss /= len(val_loader2.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # === Early stopping ===\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model5.state_dict(), 'best_model.pth')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_4hRao5mQHef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12954,
     "status": "ok",
     "timestamp": 1748055936815,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "_4hRao5mQHef",
    "outputId": "81b752aa-2f80-4d5d-a2a1-1461af91aff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Pearson r: 0.5699\n",
      "Male Pearson r: 0.6084\n",
      "Female Pearson r: 0.5233\n"
     ]
    }
   ],
   "source": [
    "model5.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_genders = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_indices in test_loader2:\n",
    "        batch_indices = batch_indices.numpy()\n",
    "        images, bmis, genders = load_batch(batch_indices)\n",
    "\n",
    "        images = images.to(device)\n",
    "        outputs = model2(images).cpu().squeeze().numpy()\n",
    "\n",
    "        all_preds.extend(outputs)\n",
    "        all_labels.extend(bmis.squeeze().numpy())\n",
    "        all_genders.extend(genders.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_genders = np.array(all_genders)\n",
    "\n",
    "# Overall Pearson correlation\n",
    "overall_r, _ = pearsonr(all_preds, all_labels)\n",
    "print(f\"Overall Pearson r: {overall_r:.4f}\")\n",
    "\n",
    "# Pearson r for males (gender 0)\n",
    "male_mask = (all_genders == 0)\n",
    "male_r, _ = pearsonr(all_preds[male_mask], all_labels[male_mask])\n",
    "print(f\"Male Pearson r: {male_r:.4f}\")\n",
    "\n",
    "# Pearson r for females (gender 1)\n",
    "female_mask = (all_genders == 1)\n",
    "female_r, _ = pearsonr(all_preds[female_mask], all_labels[female_mask])\n",
    "print(f\"Female Pearson r: {female_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UojjBVCtRDHl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102067,
     "status": "ok",
     "timestamp": 1748056262915,
     "user": {
      "displayName": "Anusha Bhat",
      "userId": "17918010741182048907"
     },
     "user_tz": 300
    },
    "id": "UojjBVCtRDHl",
    "outputId": "d634d9d8-8da4-467b-b3f9-1650f83d4cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Collecting facenet-pytorch\n",
      "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.32.3)\n",
      "INFO: pip is looking at multiple versions of facenet-pytorch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting facenet-pytorch\n",
      "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.4.26)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, facenet-pytorch\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed facenet-pytorch-2.5.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kzBt_BEAS0fG",
   "metadata": {
    "id": "kzBt_BEAS0fG"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
